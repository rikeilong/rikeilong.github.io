<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN">
<HTML xml:lang="en" xmlns="http://www.w3.org/1999/xhtml">
<META name=GENERATOR content="MSHTML 11.00.9600.17280"></HEAD>
<META name=keywords 
content="Qilang Ye, Qilang Ye, qilang ye, qilang ye, qilang, qilang, 叶启朗, GBU, Great Bay University">
<META 
content="IE=7.0000" http-equiv="X-UA-Compatible">
<head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <link rel="stylesheet" href="./jemdoc.css" type="text/css">
    <title>Qilang's Homepage</title>
</head>



<body data-feedly-mini="yes">

<div id="layout-content" style="margin-top:25px">

<table>
    <tbody>
        <tr>
            <td width="670" >
                <div id="toptitle">                 
                    <h1>Qilang Ye (叶启朗) &nbsp; </h1><h1> <!-- <img src="./zzz_files/zizhao.jpg" width="190" style="margin-bottom:-10px"> -->
                </h1></div>
                <p>
                    <h3>Research Assistant
                    </h3></div> 
                    <br>
                    <a href="https://cc.nankai.edu.cn/">College of Computer Science</a>, 
                    <br>
                    <br>
                    <a href="https://cc.nankai.edu.cn/">Nankai University</a>
                    <br>
                    <br>
                    Email: rikeilong[AT]gmail.com
                    <br>
                    <br>
                    [<a href="https://scholar.google.com/citations?user=1joiJpUAAAAJ&hl=zh-CN">Google Scholar</a>]
                    [<a href="https://github.com/rikeilong">GitHub</a>]              
                </p>
            </td>
            <td rowspan="1">
                <img src="./linkfile/portrait/qilang.png" border="0" width="400" align="bottom" ><br>
            </td>
        </tr>
                <!-- <br> -->
                <!-- <colgroup><col height="50" width="270">
                </colgroup> -->
                <!-- <tbody> -->
<!--                 <td align="bottom" colspan="2" style="margin-top:2px">
                    <h2>News</h2>
                    <div style="height: 250px; overflow: auto;">
                    <ul style="margin-left:2px; padding-left:20px; margin-top:5px">
                        <li>10/2024: I have one paper about indoor action recognition accepted by <a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=97"> IEEE Signal Processing Letters</a>. Cheers!
                        <li>07/2024: One paper about Audio-Visual Multimodal Large Language Model accepted by <a href="https://eccv.ecva.net/">ECCV 2024</a>.
                        <li>01/2024: I have one paper about action recognition has been accepted by <a href="https://digital-library.theiet.org/content/journals/iet-ipr"> IET Image Processing</a>.                 
                    </ul>
                    </div>
                </td> -->
        </tbody>
</table>

<h2>About Me</h2>
<p>
    I am currently a PhD candidate at Nankai University. My research interests include <b> Multimodal Learning</b>, and <b> Multimodal Large Language Models</b>. Recently, I am working on alignment learning to optimize LLMs outputs, e.g. <b> audio-visual hallucinations</b>, <b>ambiguity</b> , etc.
<p>
<!--     I am an incoming Ph.D. student at Nankai University and will be joining the <a href="https://intimelab.github.io/">InTimeLab</a>. -->
    
    <!-- I am interested in Robustness (e.g., Fake News Detection, DeepFake Detection, Face Anti-spoofing, Adversarial Attacks/Defense) and Generalization (e.g., Domain Adaptation/Generalization, Meta Learning, and Federated Learning) of Computer Vision and Machine Learning.</p><br>  -->
<!--     My current research focuses on exploiting the adaptive and generalized deep learning to develop a secure face recognition system with good generalization ability. -->

<!--     <p style='color:red'><strong> I am looking for a postdoctoral position or a full-time job. Please feel free to drop me an email.</strong></p> -->
</p> 
    
<h2>Publications (* co-first authors) </a>  </h2> 
<table id="tbPublications" width="100%">

    <tr>    
        <td width="300">
            <img src="./linkfile/ECCV2024/framework.png" width="260px" height="160px">
            </td>
            <td>
        <p> CAT: Enhancing Multimodal Large Language Model to Answer Questions in Dynamic Audio-Visual Scenarios </a></p>
        <b>Qilang Ye</b>, Zitong Yu, Rui Shao, Xinyu Xie, Philip Torr, Xiaochun Cao</p>
        <p><i>European Conference on Computer Vision (<b>ECCV</b>), 2024. </i></p>
        [<a href="https://arxiv.org/abs/2403.04640">paper</a>]
        [<a href="https://github.com/rikeilong/Bay-CAT">Code</a>]
    </tr> 
    <tr><td>   <br>  </td></tr>
    
    <tr>    
        <td width="300">
            <img src="./linkfile/TPAMI/framework.pdf" width="260px" height="160px">
            </td>
            <td>
        <p> CAT+: Investigating and Enhancing Audio-visual Understanding in Large Language Models  </a></p>
        <b>Qilang Ye</b>, Zitong Yu, Xin Liu</p>
        <p><i>IEEE Transactions on Pattern Analysis and Machine Intelligence (<b>TPAMI</b>), 2025.3582389</i></p>
        [<a href="https://ieeexplore.ieee.org/abstract/document/11050020">paper</a>]
    </tr> 
    <tr><td>   <br>  </td></tr>
    
    <tr>    
        <td width="300">
            <img src="./linkfile/SPL/framework.png" width="260px" height="160px">
            </td>
            <td>
        <p> Pose-promote: Progressive Visual Perception for Activities of Daily Living </a></p>
        <b>Qilang Ye</b>, Zitong Yu</p>
        <p><i>IEEE Signal Processing Letters (<b>IEEE SPL</b>) </i></p>
        [<a href="https://ieeexplore.ieee.org/abstract/document/10716484">paper</a>]
        [<a href="https://github.com/rikeilong/Ppromo-IAR">Code</a>]
        <!-- [<a href="https://cybertronagent.github.io/Optimus-1.github.io/">Project Page</a>] -->
    </tr> 
    <tr><td>   <br>  </td></tr>

    <tr>    
        <td width="300">
            <img src="./linkfile/IET/framework.png" width="260px" height="90px">
            </td>
            <td>
        <p> 3sG: Three-stage Guidance for Indoor Human Action Recognition </a></p>
        Hai Nan*, <b>Qilang Ye</b>*, Zitong Yu, Kang An</p>
        <p><i>IET Image Processing </i></p>
        [<a href="https://doi.org/10.1049/ipr2.13078">paper</a>]
        [<a href="">Code</a>]
    </tr> 
    <tr><td>   <br>  </td></tr>

    <tr>    
        <td width="300">
            <img src="./linkfile/chinese/framework.png" width="260px" height="160px">
            </td>
            <td>
        <p> 一种基于人体骨架的任意角度坐姿识别方法  </a></p>
        <b>Qilang Ye</b>, Hai Nan, Daixin Li</p>
        <p><i>中文核心</i></p>
        [<a href="https://www.arocmag.com/abs/2023.03.0070">paper</a>]
    </tr> 
    <tr><td>   <br>  </td></tr>

    <!-- <tr>
            <td width="300">
            <img src="./linkfile/tracvid2016.jpg" width="200px" > 
            </td>
            <td>
            <p> <a href="https://www-nlpir.nist.gov/projects/tvpubs/tv15.papers/nercms.pdf"> WHU-NERCMS at TREVCID 2015:Instance search task </a></p>
            Lei Yao, Mang Ye, Dongjing Liu, <b>Rui Shao</b>, Tao Liu, Jun Liu, Zheng Wang, Chao Liang.</p>
            <p><i> Participant Notebook Paper. (<b>TRECVID</b>), 2015</i>

            </p></td>
    <tr><td>   <br>  </td></tr> -->

 
</tbody></table>

<!-- <h2>Ongoing Works </a>  </h2> 
<table id="tbPublications" width="100%">

    <tr>    
        <td width="300">
            <img src="./linkfile/ongoing/TPAMI/framework.png" width="260px" height="160px">
            </td>
            <td>
        <p> CAT+: Investigating and Enhancing Audio-visual Understanding in Large Language Models </a></p>
        <b>Qilang Ye</b>, Zitong Yu, Rui Shao, Yawen Cui, Xiangui Kang, Xin Liu, Philip Torr, Xiaochun Cao</p>
        <p><i>IEEE Transactions on Pattern Analysis and Machine Intelligence (<b>TPAMI</b>) (Under Review)</i></p>
    </tr> 
    <tr><td>   <br>  </td></tr>

    <tr>    
        <td width="300">
            <img src="./linkfile/ongoing/WACV/framework.png" width="260px" height="160px">
            </td>
            <td>
        <p> SUGAR: Learning Skeleton Representation with Visual-Motion Knowledge for Action Recognition </a></p>
        <b>Qilang Ye</b>, Yu Zhou, Xiangui Kang, Mingkui Tan, Weicheng Xie, Linlin Shen, Yue Sun, Tao Tan, Zitong Yu</p>
        <p><i>Computer Vision and Pattern Recognition 2025 (<b>CVPR</b>) (Under Review)</i></p>
    </tr> 
    <tr><td>   <br>  </td></tr>

    <tr>    
        <td width="300">
            <img src="./linkfile/ongoing/PR/framework.png" width="260px" height="160px">
            </td>
            <td>
        <p> CueNet: A Cue-Aware Network for Audio-Visual Question Answering </a></p>
        <b> Qilang Ye</b>, Zitong Yu, Xin Liu</p>
        <p><i>Pattern Recognition (<b>PR</b>) (Under Review)</i></p>
        <!-- [<a href="https://cybertronagent.github.io/Optimus-1.github.io/">Project Page</a>] -->
    
   <!--  </tr> 
    <tr><td>   <br>  </td></tr>


    <tr>    
        <td width="300">
            <img src="./linkfile/ongoing/IF/framework.png" width="260px" height="160px">
            </td>
            <td>
        <p> SDFformer: Similarity-Guided Dynamic Frequency Transformers for Multimodal Medical Images Fusion </a></p>
        Xinyu Xie, Zitong Yu, <b>Qilang Ye</b>, Xubin Zheng, Xiaozhi Zhang</p>
        <p><i>IEEE Transactions on Instrumentation and Measurement (<b>IEEE TIM</b>) (Under Review)</i></p>
    </tr> 
    <tr><td>   <br>  </td></tr>

    <tr>    
        <td width="300">
            <img src="./linkfile/ongoing/AAAI/framework.png" width="260px" height="160px">
            </td>
            <td>
        <p> EMO-LLaMA: Enhancing Facial Emotion Understanding with Instruction Tuning </a></p>
        Bohao Xing, Zitong YU, Xin Liu, Kaishen Yuan, <b>Qilang Ye</b>, Linlin Shen, Huanjing Yue, Jingyu Yang</p>
        <p><i>Association for the Advancement of Artificial Intelligence 2025 (<b>AAAI</b>) (Under Review)</i></p>
    </tr> 
    <tr><td>   <br>  </td></tr> -->

    <!-- <tr>
            <td width="300">
            <img src="./linkfile/tracvid2016.jpg" width="200px" > 
            </td>
            <td>
            <p> <a href="https://www-nlpir.nist.gov/projects/tvpubs/tv15.papers/nercms.pdf"> WHU-NERCMS at TREVCID 2015:Instance search task </a></p>
            Lei Yao, Mang Ye, Dongjing Liu, <b>Rui Shao</b>, Tao Liu, Jun Liu, Zheng Wang, Chao Liang.</p>
            <p><i> Participant Notebook Paper. (<b>TRECVID</b>), 2015</i>

            </p></td>
    <tr><td>   <br>  </td></tr>

 
</tbody></table>

<!-- <h2>Awards</h2>
<table id="Awards" border="0" width="100%">

    <tbody>
    <li><a href="http://www.comp.hkbu.edu.hk/v1/?pid=48"> 2018/19 Computer Science Department RPg Performance Award</a> 
    <li><a href="http://www.comp.hkbu.edu.hk/v1/?pid=48"> 2017/18 Computer Science Department RPg Performance Award</a>

<tr><td>   <br>  </td></tr> -->


<!-- <h2>Services</h2>
<table id="Services" border="0" width="100%">

    <tbody>

Area Chair:
<li>ACM MM 2024
<li>BMVC 2024

<!-- Invited Reviewer for: 
    <li>TPAMI, IJCV, TIP, TNNLS, TIFS, TCSVT, JSTSP, PR
    <li>CVPR, ICCV, ECCV, ICML, NeurIPS, ICLR, AAAI, IJCAI, ACM MM
    <li>Program Committee Member for: AAAI 2021 2022 2023 -->

<!-- <tr><td>   <br>  </td></tr>

</tbody></table> 


<h2></h2> -->

<!-- <a href="https://www.easycounter.com/">
<img src="https://www.easycounter.com/counter.php?rshaojimmy"
border="0" alt="Web Site Hit Counter"></a>
<br><a href="https://www.easycounter.com/">Unique visitors since Apr 2019</a>

<!-- <script type="text/javascript" src="//ra.revolvermaps.com/0/0/8.js?i=0t4x80je5xv&amp;m=0&amp;c=baff00&amp;cr1=baff00&amp;f=arial&amp;l=33" async="async"></script> -->

<!-- <script type='text/javascript' id='clustrmaps' src='//cdn.clustrmaps.com/map_v2.js?cl=ffffff&w=400&t=n&d=yQtd-EynvtzIDuSGynEuIlVZWeRz8dMLMy3lDGuyPAc'></script> --> 

<!-- <div id="footer">
    <div id="footer-text"></div> -->

</div>
</body>
</html>
